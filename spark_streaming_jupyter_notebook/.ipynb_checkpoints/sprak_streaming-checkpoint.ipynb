{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550befb8-83c9-4e7d-aab3-a907c552dca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Trend Analysis on Twitter live data using Spark Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd540e-465a-486c-b7ea-82fad6aed3c0",
   "metadata": {},
   "source": [
    "Import sparkContext & StreamingContext from PySpark library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c8689f-9adb-4acf-bc77-55cf16ad7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce7606-cc71-4dda-b9ed-3968cf1ffe51",
   "metadata": {},
   "source": [
    "Create a sparkContext with AppName  \"StreamingTwitterAnalysis\".\n",
    "<br>Setting the LogLevel of SparkContext to ERROR. This will not print all the logs which are INFO or WARN level.\n",
    "<br>Create Spark Streaming Context using SC (spark context).parameter 10 is the batch interval.\n",
    "<br>Every 10 second the analysis will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4b256b-1426-4a7d-9697-b19ae1b17578",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"abc\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcd738-9404-4760-9a0e-2df7274d2745",
   "metadata": {},
   "source": [
    "Connect to socket broker using ssc (spark streaming context)\n",
    "<br>Host:\"172.16.117.15\"(localhost) & port:5555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c18c48-a939-4772-9982-bfa710056b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"172.16.117.15\", 6677)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefff73a-a4cb-4ab5-bc82-59666d5c6f98",
   "metadata": {},
   "source": [
    "window function parameter sets the Window length. All the analysis will be done on tweets stored for 20 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd740901-c367-4707-95f6-2d1bdb712ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b78767-d4fa-4748-99a2-0fae6fe5ea24",
   "metadata": {},
   "source": [
    "### **Process the Stream:**\n",
    "1. Receives tweet message, stored in lines. **Input DStream**\n",
    "2. splits the messages into words. **Apply transformation on DStream : flatMap**\n",
    "3. filters all the words which start with a hashtag(#). **transformation : filter**\n",
    "4. converts the words to lowercase. **transformation : map**\n",
    "5. maps each tag to (word, 1). **transformation : map**\n",
    "6. then reduces and counts occurrences of each hash tag. (action : reduceByKey) hashtags = **output DStream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07e97a7-33d8-46f0-8be6-4ec23a9b22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags= lines.flatMap(lambda text: text.split(\" \")).filter(lambda word: word.lower().startswith('#')).map( lambda word:( word.lower(),1)).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9594a9c-8238-4c14-a7eb-be0daaef5dd6",
   "metadata": {},
   "source": [
    "Sort the hashtags based on the counts in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0fe042-fc59-4959-a893-990bb64f81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream = hashtags.transform(lambda foo:foo.sortBy(lambda x:x[0].lower()).sortBy(lambda x:x[1], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f09f42-57b3-4de9-b1bd-c88fbc20d043",
   "metadata": {},
   "source": [
    "Print the final aalysis: Most popular hashtags on streaming twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2951ef9f-122d-444c-80df-27423d91ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add7326-fe77-415c-8fae-e1318c6990dc",
   "metadata": {},
   "source": [
    "## **Starting the Spark Streaming:**\n",
    "Spark Streaming code we have written till now will not execute, untill we start the ssc.\n",
    "<br>ssc.start() will start the spark streaming context. This is the Action for the whole code.\n",
    "<br>Now it'll create the lineage & DAG & do the lazy evaluation & start running the whole sequence of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e29503-f7ab-45f1-9bb9-5dd73a3d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51de8f0-61aa-4eb9-969b-b7f5a52bdcda",
   "metadata": {},
   "source": [
    "awaitTermination() is very important to stop the SSC\n",
    "<br> When we kill this python process then this signal will be sent to awaitTermination() function.\n",
    "<br> It will finally stop the spark streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd99d73-9193-4e38-bdc0-8f5d127ab121",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.awaitTermination.\n: java.net.SocketException: Connection reset\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n\tat py4j.ClientServerConnection.readBlockingResponse(ClientServerConnection.java:313)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:229)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy19.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6cfd70f7bded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.awaitTermination.\n: java.net.SocketException: Connection reset\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n\tat py4j.ClientServerConnection.readBlockingResponse(ClientServerConnection.java:313)\n\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:229)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n\tat com.sun.proxy.$Proxy19.call(Unknown Source)\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    }
   ],
   "source": [
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a11d3d-5ca9-4584-a53f-e5812b8a6e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
